<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Bayesian inference methods • jSDM</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Bayesian inference methods">
<meta property="og:description" content="jSDM">
<meta property="og:image" content="/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">jSDM</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.1.1</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/jSDM.html">Get started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li>
  <a href="../articles/index.html">Articles</a>
</li>
<li>
  <a href="../news/index.html">Change log</a>
</li>
<li>
  <a></a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/ghislainv/jSDM">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="proof_files/jquery-1.11.3/jquery.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1">
<link href="proof_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet">
<script src="proof_files/bootstrap-3.3.5/js/bootstrap.min.js"></script><script src="proof_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script><script src="proof_files/bootstrap-3.3.5/shim/respond.min.js"></script><style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="proof_files/navigation-1.1/tabsets.js"></script><script src="proof_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Bayesian inference methods</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/ghislainv/jSDM/blob/master/vignettes/proof.Rmd"><code>vignettes/proof.Rmd</code></a></small>
      <div class="hidden name"><code>proof.Rmd</code></div>

    </div>

    
    
<div id="bernoulli-distribution-with-probit-link-function" class="section level1">
<h1 class="hasAnchor">
<a href="#bernoulli-distribution-with-probit-link-function" class="anchor"></a><span class="header-section-number">1</span> Bernoulli distribution with probit link function</h1>
<div id="model-definition" class="section level2">
<h2 class="hasAnchor">
<a href="#model-definition" class="anchor"></a><span class="header-section-number">1.1</span> Model definition</h2>
<p>According to the article <span class="citation">Albert &amp; Siddhartha (<a href="#ref-Albert1993" role="doc-biblioref">1993</a>)</span>, a possible model is to assume the existence of an underlying latent variable related to our observed binary variable using the following proposition :</p>
<ul>
<li>Proposition</li>
</ul>
<p><span class="math display">\[ 
\begin{aligned}
&amp;z_{i,j} = \alpha_i + \beta_{0j}+X_i'\beta_j+ W_i'\lambda_j + \epsilon_{i,j},\\
&amp;\text{ with } \epsilon_{i,j} \sim \mathcal{N}(0,1) \ \forall i,j  \text{ and such as : } \\
&amp;y_{i,j}=
\begin{cases}
1 &amp; \text{ if } z_{i,j} &gt; 0 \\
0 &amp;  \text{ otherwise.}
\end{cases} 
\end{aligned}
\Rightarrow  
\begin{cases}
y_{i,j}| z_{i,j} \sim \mathcal{B}ernoulli(\theta_{i,j}) \text{ with } \\
\theta_{i,j} = \Phi(\alpha_i + \beta_{0j}+X_i'\beta_j+ W_i'\lambda_j) \\
\text{where } \Phi \text{ correspond to the repartition function} \\
\text{of the reduced centred normal distribution.}
\end{cases}
\]</span></p>
<ul>
<li>Proof</li>
</ul>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}(y_{i,j}=1) &amp; = \mathbb{P}(z_{i,j} &gt; 0)\\
&amp; = \mathbb{P}(\alpha_i + \beta_{0j}+X_i'\beta_j+ W_i'\lambda_j + \epsilon_{i,j} &gt; 0)\\
&amp; = \mathbb{P}(\epsilon_{i,j} &gt; - (\alpha_i + \beta_{0j} + X_i'\beta_j + W_i'\lambda_j) \ ) \\
&amp; = \mathbb{P}(\epsilon_{i,j} \leq \alpha_i + \beta_{0j} + X_i'\beta_j + W_i'\lambda_j) \\
&amp; = \Phi( \alpha_i + \beta_{0j} + X_i'\beta_j + W_i'\lambda_j) \\
\end{aligned}\]</span></p>
<p>In the same way:</p>
<p><span class="math display">\[\begin{aligned}
\mathbb{P}(y_{i,j}=0) &amp; = \mathbb{P}(z_{i,j} \leq 0)\\
&amp; = \mathbb{P}(\epsilon_{i,j} \leq - (\alpha_i + \beta_{0j} + X_i'\beta_j + W_i'\lambda_j) \ ) \\
&amp; = \mathbb{P}(\epsilon_{i,j} &gt; \alpha_i + \beta_{0j} + X_i'\beta_j + W_i'\lambda_j) \\
&amp; = 1 - \Phi( \alpha_i + \beta_{0j} + X_i'\beta_j + W_i'\lambda_j) \\
\end{aligned}\]</span></p>
<p>with the following parameters and priors :</p>
<ul>
<li><p>Latent variables: <span class="math inline">\(W_i=(W_i^1,\ldots,W_i^q)\)</span> where <span class="math inline">\(q\)</span> is the number of latent variables considered, which has to be fixed by the user (by default q=2).
We assume that <span class="math inline">\(W_i \sim \mathcal{N}(0,I_q)\)</span> and we define the associated coefficients: <span class="math inline">\(\lambda_j=(\lambda_j^1,\ldots, \lambda_j^q)'\)</span>. We use a prior distribution <span class="math inline">\(\mathcal{N}(0,10)\)</span> for all lambdas not concerned by constraints to <span class="math inline">\(0\)</span> on upper diagonal and to strictly positive values on diagonal.</p></li>
<li><p>Explanatory variables: bioclimatic data about each site. <span class="math inline">\(X=(X_i)_{i=1,\ldots,nsite}\)</span> with <span class="math inline">\(X_i=(x_i^1,\ldots,x_i^p)\in \mathbb{R}^p\)</span> where <span class="math inline">\(p\)</span> is the number of bioclimatic variables considered.
The corresponding regression coefficients for each species <span class="math inline">\(j\)</span> are noted : <span class="math inline">\(\beta_j=(\beta_j^1,\ldots,\beta_j^p)'\)</span>.</p></li>
<li><p><span class="math inline">\(\beta_{0j}\)</span> correspond to the intercept for species <span class="math inline">\(j\)</span> which is assume to be a fixed effect. We use a prior distribution <span class="math inline">\(\mathcal{N}(0,10^6)\)</span> for all betas.</p></li>
<li><p><span class="math inline">\(\alpha_i\)</span> represents the random effect of site <span class="math inline">\(i\)</span> such as <span class="math inline">\(\alpha_i \sim \mathcal{N}(0,V_{\alpha})\)</span> and we assumed that <span class="math inline">\(V_{\alpha} \sim \mathcal {IG}(\text{shape}=0.5, \text{rate}=0.005)\)</span> as prior distribution by default.</p></li>
</ul>
</div>
<div id="conjugate-priors" class="section level2">
<h2 class="hasAnchor">
<a href="#conjugate-priors" class="anchor"></a><span class="header-section-number">1.2</span> Conjugate priors</h2>
<div id="fixed-species-effects" class="section level3">
<h3 class="hasAnchor">
<a href="#fixed-species-effects" class="anchor"></a><span class="header-section-number">1.2.1</span> Fixed species effects</h3>
<ul>
<li>Proposition</li>
</ul>
<p>We go back to a model of the form: <span class="math inline">\(Z' = X\beta + \epsilon\)</span> to estimate the posterior distributions of betas, lambdas and latent variables <span class="math inline">\(W_i\)</span> of the model. For example concerning <span class="math inline">\(\lambda_j\)</span>, we define <span class="math inline">\(Z'_{i,j} = Z_{i,j} - \alpha_i - \beta_{0j} - X_i'\beta_j\)</span> such as <span class="math inline">\(Z'_{i,j} = W_i'\lambda_j + \epsilon_{i,j}\)</span> so <span class="math inline">\(Z'_{i,j} \ | \ W_i \ , \ \lambda_j \  \sim \mathcal{N}( W_i'\lambda_j, 1)\)</span>.</p>
<p>In this case we can use the following proposition:</p>
<p><span class="math display">\[\begin{cases} 
Y \ | \ \beta &amp;\sim \mathcal{N}_n ( X\beta, I_n) \\
\beta  &amp;\sim \mathcal{N}_p (m,V)
\end{cases}
\Rightarrow \begin{cases}
\beta|Y &amp;\sim \mathcal{N}_p (m^*,V^*) \text{ with }  \\
m^* &amp;= (V^{-1} + X'X)^{-1}(V^{-1}m + X'Y)\\
V^*&amp;=(V^{-1} + X'X)^{-1} 
\end{cases}\]</span>.</p>
<ul>
<li>Proof</li>
</ul>
<p><span class="math display">\[\begin{aligned}
p(\beta \ | \ Y) &amp; \propto  p(Y \ | \ \beta) \ p(\beta) \\
&amp; \propto  \frac{1}{(2\pi)^{\frac{n}{2}}}\exp\left(-\frac{1}{2}(Y-X\beta)'(Y-X\beta)\right)\frac{1}{(2\pi)^{\frac{p}{2}}|V|^{\frac{1}{2}}}\exp\left(-\frac{1}{2}(\beta-m)'V^{-1}(\beta-m)\right) \\
&amp; \propto \exp\left(-\frac{1}{2}\left((\beta-m)'V^{-1}(\beta-m) + (Y-X\beta)'(Y-X\beta)\right)\right) \\
&amp; \propto \exp\left(-\frac{1}{2}\left(\beta'V^{-1}\beta + m'V^{-1}m - m'V^{-1}\beta -\beta'V^{-1}m + Y'Y + \beta'X'X\beta - Y'X\beta - \beta'X'Y\right)\right) \\
&amp; \propto \exp\left(-\frac{1}{2}\left(\beta'(V^{-1}+X'X)\beta -\beta'(V^{-1}m + X'Y) - (Y'X + m'V^{-1})\beta + m'V^{-1}m + Y'Y \right)\right) \\
&amp; \propto \exp\left(-\frac{1}{2}\left(\beta'(V^{-1}+X'X)\beta -\beta'(V^{-1}m + X'Y) - (X'Y + V^{-1}m)'\beta + m'V^{-1}m + Y'Y \right)\right) \\
&amp; \propto \exp(-\frac{1}{2}\left(\beta - (V^{-1}+X'X)^{-1}(V^{-1}m + X'Y)\right)'(V^{-1}+X'X)\left(\beta - (V^{-1}+X'X)^{-1}(V^{-1}m + X'Y)\right)\\
&amp; \quad -(V^{-1}m + X'Y)'(V^{-1}+X'X)^{-1}(V^{-1}m + X'Y) +m'V^{-1}m + Y'Y)\\
&amp; \propto \exp\left(-\frac{1}{2}\left(\beta - \underbrace{(V^{-1}+X'X)^{-1}(V^{-1}m + X'Y)}_{m^*}\right)'\underbrace{(V^{-1}+X'X)}_{{V^*}^{-1}}\left(\beta - (V^{-1}+X'X)^{-1}(V^{-1}m + X'Y)\right)\right)
\end{aligned}\]</span></p>
<p>Actually, we use that proposition to estimate lambdas and betas in a single block. So, we consider <span class="math inline">\(Z'_{i,j} = \beta_{0j} + X_i'\beta_j+ W_i'\lambda_j +\epsilon_{i,j}\)</span>.</p>
</div>
<div id="random-site-effects" class="section level3">
<h3 class="hasAnchor">
<a href="#random-site-effects" class="anchor"></a><span class="header-section-number">1.2.2</span> Random site effects</h3>
<ul>
<li>Proposition</li>
</ul>
<p>About the posterior distribution of the random site effects <span class="math inline">\((\alpha_i)_{i=1,\dots,nsite}\)</span>, we can use a transformation of the form <span class="math inline">\(Z'_{i,j} = \alpha_i + \epsilon_{i,j}\)</span>, with <span class="math inline">\(Z'_{i,j} = Z_{i,j} - W_i'\lambda_j - X_i'\beta_j- \beta_{0j}\)</span> so <span class="math inline">\(Z'_{i,j} \ | \ W_i \ , \ \lambda_j, \ \beta_j, \ \beta_{0j}, \ \alpha_i \ \sim \mathcal{N}(\alpha_i,1)\)</span>. We then use the following proposition:</p>
<p><span class="math display">\[\begin{cases} 
x \ | \ \theta &amp; \sim \mathcal{N}(\theta, \ \sigma^2) \\
\theta  &amp; \sim \mathcal{N}(\mu_0,{\tau_0}^2) \\
\sigma^2 &amp; \text{ known}
\end{cases}
\Rightarrow
\begin{cases} 
\theta | \ x &amp;\sim \mathcal{N}(\mu_1,{\tau_1}^2) \text{ with }\\
\mu_1 &amp;= \dfrac{{\tau_0}^2\mu_0 + x\sigma^2}{{\tau_0}^{-2}+\sigma^{-2}} \\
{\tau_1}^{-2} &amp;={\tau_0}^{-2}+\sigma^{-2}
\end{cases}\]</span>.</p>
<ul>
<li>Proof</li>
</ul>
<p><span class="math display">\[\begin{aligned}
p(\theta \ | \ x) &amp; \propto  p(x \ | \ \theta) \ p(\theta) \\
&amp; \propto  \frac{1}{(2\pi\sigma^2)^{\frac{1}{2}}}\exp\left(-\frac{1}{2\sigma^2}(x-\theta)^2\right)\frac{1}{(2\pi{\tau_0}^2)^{\frac{1}{2}}}\exp\left(-\frac{1}{2{\tau_0}^2}(\theta-\mu_0)^2\right) \\
&amp; \propto \exp\left(-\frac{1}{2{\tau_0}^2}(\theta-\mu_0)^2-\frac{1}{2\sigma^2}(x-\theta)^2\right) \\
&amp; \propto \exp\left(-\frac{1}{2{\tau_0}^2}(\theta^2-2\mu_0\theta)-\frac{1}{2\sigma^2}(\theta^2-2x\theta)\right)\\
&amp; \propto \exp\left(-\frac{1}{2}\left(\theta^2 ({\tau_0}^{-2}+\sigma^{-2})-2\mu_0\theta{\tau_0}^{-2}-2x\theta\sigma^{-2}\right)\right)\\
&amp; \propto \exp\left(-\frac{1}{2({\tau_0}^{-2}+\sigma^{-2})^{-1}}\left(\theta^2 -2\theta \frac{\mu_0{\tau_0}^{-2}+ x\sigma^{-2}}{{\tau_0}^{-2}+\sigma^{-2}}\right)\right)\\
\end{aligned}\]</span></p>
</div>
<div id="random-site-effect-variance" class="section level3">
<h3 class="hasAnchor">
<a href="#random-site-effect-variance" class="anchor"></a><span class="header-section-number">1.2.3</span> Random site effect variance</h3>
<ul>
<li>Proposition</li>
</ul>
<p>Concerning posterior distribution of <span class="math inline">\(V_{\alpha}\)</span>, the variance of random site effects <span class="math inline">\((\alpha_i)_{i=1,\dots,nsite}\)</span>, we use the following proposition :<br>
If <span class="math display">\[\begin{cases} 
x \ | \ \sigma^2 &amp; \sim \mathcal{N}_n (\theta, \ \sigma^2I_n) \\
\sigma^2  &amp; \sim \mathcal{IG} (a,b) \\
\theta &amp; \text{ known}
\end{cases} \Rightarrow 
\begin{cases}
\sigma^2|x \sim \mathcal{IG}(a',b') \text{ with } \\
a' = a + \frac{n}{2} \\ 
b' = \frac{1}{2}\sum\limits_{i=1}^n(x_i-\theta)^2 + b. 
\end{cases}\]</span></p>
<ul>
<li>Proof</li>
</ul>
<p><span class="math display">\[\begin{aligned}
p(\sigma^2 \ | \ x) &amp; \propto  p(x \ | \ \sigma^2) \ p(\sigma^2) \\
&amp; \propto  \frac{1}{(2\pi\sigma^2)^{\frac{n}{2}}}\exp\left(-\frac{1}{2\sigma^2}(x-\theta)'(x-\theta)\right)\frac{b^a}{\Gamma(a)}{(\sigma^2)}^{-(a+1)}\exp\left(-\frac{b}{\sigma^2}\right) \\
&amp; \propto {(\sigma^2)}^{-\left(\underbrace{\frac{n}{2}+a}_{a'}+1\right)}\exp\left(-\frac{1}{\sigma^2}\underbrace{\left(b+\frac{1}{2}\sum\limits_{i=1}^n(x_i-\theta)^2\right)}_{b'}\right)
\end{aligned}\]</span></p>
</div>
</div>
<div id="gibbs-sampler-principle" class="section level2">
<h2 class="hasAnchor">
<a href="#gibbs-sampler-principle" class="anchor"></a><span class="header-section-number">1.3</span> Gibbs sampler principle</h2>
<p>In the Bayesian framework, Gibbs’ algorithm produces a realization of the parameter <span class="math inline">\(\theta=(\theta_),\ldots,\theta_m)\)</span> according to the <em>a posteriori</em> law <span class="math inline">\(\Pi(\theta \ | \ x)\)</span> as soon as we are able to express the conditional laws: <span class="math inline">\(\Pi(\theta_i | \theta_1,\dots,\theta_{i-1},\theta_{i+1},\ldots,\theta_m, x)\)</span> for <span class="math inline">\(i =1,\ldots,m\)</span>.</p>
<p><strong>Gibbs sampling</strong> consists of:</p>
<ul>
<li><p><strong>Initialization</strong> : arbitrary choice of <span class="math inline">\(\theta^{(0)}= (\theta_1^{(0)},\dots,\theta_m^{(0)})\)</span>.</p></li>
<li>
<p><strong>Iteration <span class="math inline">\(t\)</span></strong> : Genererate <span class="math inline">\(\theta^{(t)}\)</span> as follows :</p>
<ul>
<li><p><span class="math inline">\(\theta_1^{(t)} \sim \Pi\left(\theta_1 \ | \theta_2^{(t-1)},\dots, \theta_m^{(t-1)}, x \right)\)</span></p></li>
<li><p><span class="math inline">\(\theta_2^{(t)} \sim \Pi\left((\theta_2 \ | \ (\theta_1^{(t)}, \theta_3^{(t-1)},\ldots,\theta_m^{(t-1)},x\right)\)</span></p></li>
<li><p><span class="math inline">\(\theta_m^{(t)} \sim \Pi\left(\theta_m \ | \ \theta_1^{(t)}, \ldots, \theta_{m-1}^{(t)},x\right)\)</span></p></li>
</ul>
</li>
</ul>
<p>Successive iterations of this algorithm generate the states of a Markov chain <span class="math inline">\(\{\theta^{(t)}, t &gt; 0\}\)</span> to values in <span class="math inline">\(\mathbb{R}^{m}\)</span>, we show that this chain admits an invariant measure which is the <em>a posteriori</em> law.</p>
<p>For a sufficiently large number of iterations, the vector <span class="math inline">\(\theta\)</span> obtained can thus be considered as a realization of the joint <em>a posteriori</em> law <span class="math inline">\(\Pi(\theta \ | \ x)\)</span>.</p>
<p>Consequently, the implementation of a Gibbs sampler requires the knowledge of the <em>a posteriori</em> distributions of each of the parameters conditionally to the other parameters of the model, which can be deduced from the conjugated priors formulas in the case of the probit model but are not explicitly expressible in the case where a logit or log link function is used.</p>
</div>
<div id="gibbs-sampler-using-conjuate-priors" class="section level2">
<h2 class="hasAnchor">
<a href="#gibbs-sampler-using-conjuate-priors" class="anchor"></a><span class="header-section-number">1.4</span> Gibbs sampler using conjuate priors</h2>
<p>The algorithm used in  function to estimate the parameters of the probit model is therefore as follows:</p>
<ul>
<li>Define the constants <span class="math inline">\(N_{Gibbs}\)</span>, <span class="math inline">\(N_{burn}\)</span>, <span class="math inline">\(N_{thin}\)</span> such that <span class="math inline">\(N_{Gibbs}\)</span> corresponds to the number of iterations performed by the Gibbs sampler, <span class="math inline">\(N_{burn}\)</span> to the number of iterations required for burn-in or warm-up time and <span class="math inline">\(N_{samp} = \dfrac{N_{Gibbs}-N_{burn}}{N_{thin}}\)</span> to the number of estimated values retained for each parameter. Indeed, the estimated parameters are recorded at certain iterations, in order to obtain a sample of <span class="math inline">\(N_{samp}\)</span> values distributed according to the <span class="math inline">\(a \ posteriori\)</span> distribution for each of the parameters.</li>
</ul>
<p>Initialize all parameters to <span class="math inline">\(0\)</span> for example, except the diagonal values of <span class="math inline">\(\Lambda\)</span> initialized at <span class="math inline">\(1\)</span> and <span class="math inline">\(V_{\alpha}^{(0)}=1\)</span>.</p>
<ul>
<li>
<p>Gibbs sampler: at each iteration <span class="math inline">\(t\)</span> for <span class="math inline">\(t=1,\ldots,N_{Gibbs}\)</span> we repeat each of these steps :</p>
<ul>
<li><p>Generate the <strong>latent variable</strong> <span class="math inline">\(Z^{(t)}=\left(Z_{ij}^{(t)}\right)_{i=1,\ldots,I}^{j=1,\ldots,J}\)</span> such that
<span class="math display">\[Z_{ij}^{(t)} \sim  \begin{cases} 
\mathcal{N}\left(\alpha_i^{(t−1)} + \beta_{j0}^{(t−1)} + X_i\beta_j{(t−1)} + W_i^{(t−1)}\lambda_j^{(t−1)}, \ 1 \right) \text{ right truncated by } 0 &amp; \text{ if } y_{ij } =0 \\ 
\mathcal{N}\left(\alpha_i^{(t−1)} + \beta_{j0}^{(t−1)} + X_i\beta_j{(t−1)} + W_i^{(t−1)}\lambda_j^{(t−1)}, \ 1 \right) \text{ left truncated by } 0 &amp;        \text{ if } y_{ij} =1
\end{cases}\]</span>
, the latent variable is thus initialized at the first iteration by generating it according to these centered normal laws.</p></li>
<li><p>Generate the <strong>fixed species effects</strong> <span class="math inline">\(P_j^{(t)}=(\beta_{j0}^{(t)},\beta_{j1}^{(t)} \ldots, \beta_{jp}^{(t)},\lambda_{j1}^{(t)},\ldots, \lambda_{jq}^{(t)})'\)</span> for <span class="math inline">\(j=1,\ldots,J\)</span> such as :
<span class="math display">\[P_j^{(t)} \ | \ Z^{(t)}, W_1^{(t-1)}, \alpha_1^{(t-1)}, \ldots, W_I^{(t−1)}, \alpha_I^{(t-1)} \sim \mathcal{N}_{p+q+1}(m^\star,V^\star) \text{, with }\]</span>
<span class="math display">\[m^\star = (V^{-1} + {D^{(t)}}'D^{(t)})^{-1}(V^{-1}m + {D^{(t)}}'Z^\star_j) \text{ and } V^\star = \left(V^{-1}+{D^{(t)}}'D^{(t)}\right)^{-1},\]</span>
<span class="math display">\[\text{ where } Z_j^\star =(Z_{1j}^\star,\ldots,Z_{Ij}^\star)' \text{ such as } Z^\star_{ij} = Z_{ij}^{(t)}-\alpha_i^{(t-1)}.\]</span>
In order to constrain the diagonal values of <span class="math inline">\(\Lambda =\left(\lambda_{jl}\right)_{j=1,\ldots,J}^{l=1,\ldots,q}\)</span> to positive values and make the matrix lower triangular,
the values of the randomly simulated <span class="math inline">\(P^{(t)}\)</span> are modified according to the following conditions:
<span class="math display">\[P_{jp+1+l}^{(t)} = \lambda_{jl}^{(t)} \leftarrow \begin{cases}
0 &amp; \text{ if } l&gt;j \\
\lambda_{jl}^{(t-1)} &amp; \text{ if } l=j \text{ and } \lambda_{jl}^{(t)} &lt; 0.
\end{cases}\]</span>
We define <span class="math inline">\(P^{(t)}=\left( P_1^{(t)} | \ldots | P_J^{(t)} \right)\)</span>.</p></li>
<li><p>Generate the <strong>latent variables</strong> (or unmeasured predictors) <span class="math inline">\(W_i^{(t)}\)</span> for <span class="math inline">\(i=1,\ldots,I\)</span> according to : <span class="math display">\[W_i^{(t)} \ | \ Z^{(t)}, P^{(t)},  \alpha_i^{(t-1)} \sim \mathcal{N}_{q} \left((I_q + {\Lambda^{(t)}}'\Lambda^{(t)})^{-1}({\Lambda^{(t)}}'Z_i^{\star \star}),(I_q + {\Lambda^{(t)}}'\Lambda^{(t)})^{-1}\right),\]</span>
<span class="math display">\[\text{ where } Z_i^{\star \star} =(Z_{i1}^{\star \star},\ldots,Z_{iJ}^{\star \star}) \text{ such as } Z_{ij}^{\star \star } = Z_{ij}^{(t)}-\alpha_i^{(t-1)} − \beta_{j0}^{(t)} - X_i\beta_j^{(t)}.\]</span> We define <span class="math inline">\(D_i^{(t)} = \left(1,X_{i1},\ldots,X_{ip},W_{i1}^{(t)}, \ldots, W_{iq}^{(t)} \right)\)</span>.</p></li>
<li><p>Generate the <strong>random site effects</strong> <span class="math inline">\(\alpha_i^{(t)}\)</span> for <span class="math inline">\(i=1,\ldots,I\)</span> selon :
<span class="math display">\[ \alpha_i | \ Z^{(t)}, P^{(t)},W_i^{(t)} \sim \mathcal{N}\left(\dfrac{ \sum_{j=1}^J Z_{ij}^{(t)} - D_i^{(t)}P_j^{(t)}}{{V_{\alpha}^{(t-1)}}^{-1} + J} , \left( \frac{1}{V_{\alpha}^{(t-1)}}+ J \right)^{-1}  \right)\]</span></p></li>
<li><p>Generate the <strong>variance of random site effects</strong> <span class="math inline">\(V_\alpha^{(t)}\)</span> according to: <span class="math display">\[V_\alpha^{(t)} \ | \ \alpha_1^{(t)},\ldots,\alpha_I^{(t)} \sim \mathcal{IG}\left( \text{shape}=0.5 + \frac{I}{2}, \text{rate}=0.005 + \frac{1}{2}\sum\limits_{i=1}^I \left(\alpha_i^{(t)}\right)^2\right)\]</span></p></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="binomial-distribution-with-logit-link-function" class="section level1">
<h1 class="hasAnchor">
<a href="#binomial-distribution-with-logit-link-function" class="anchor"></a><span class="header-section-number">2</span> Binomial distribution with logit link function</h1>
<div id="model-definition-1" class="section level2">
<h2 class="hasAnchor">
<a href="#model-definition-1" class="anchor"></a><span class="header-section-number">2.1</span> Model definition</h2>
<p>In the same way as for the probit model, the logit model can be defined by means of a latent variable: <span class="math inline">\(Z_{ij}= \alpha_i + \beta_{j0} + X_i\beta_j + W_i\lambda_j + \epsilon_{ij}\)</span> for <span class="math inline">\(i=1,\ldots,I\)</span> et <span class="math inline">\(j=1,\ldots,J\)</span>, with <span class="math inline">\(\epsilon_{ij} \sim \mathrm{logistique}(0,1)\)</span> <em>iid</em> and such as:
<span class="math display">\[y_{ij}=
\begin{cases}
1 &amp; \text{ if } Z_{ij} &gt; 0 \\
0 &amp;  \text{ else }
\end{cases}\]</span>
However in this case the <em>a priori</em> distributions of the latent variable and the parameters are not conjugated, we are not able to use the properties of the conjugated priors, so modelling using a latent variable is irrelevant.<br>
In this case it is assumed that <span class="math display">\[y_{ij} \ | \theta_{ij} \sim \mathcal{B}inomial(n_i,\theta_{ij})\]</span>, with
<span class="math inline">\(\mathrm{probit(\theta_{ij})} = \alpha_i + \beta_{j0}+X_i\beta_j+ W_i\lambda_j\)</span> and <span class="math inline">\(n_i\)</span> the number of visits to the site <span class="math inline">\(i\)</span>.<br>
Therefore, the parameters of this model will be sampled by estimating their conditional <em>a posteriori</em> distributions using an adaptive Metropolis algorithm.</p>
</div>
<div id="priors-used" class="section level2">
<h2 class="hasAnchor">
<a href="#priors-used" class="anchor"></a><span class="header-section-number">2.2</span> Priors used</h2>
<p>An <em>a priori</em> distribution is determined for each of the parameters of the model :<br><span class="math display">\[\begin{array}{lll}
V_{\alpha} &amp; \sim &amp; \mathcal {IG}(\text{shape}=0.5, \text{rate}=0.005) \text{ with } \mathrm{rate}=\frac{1}{\mathrm{scale}}, \\
\beta_{jk} &amp; \sim &amp; \mathcal{N}(0,10^6)  \text{ for } j=1,\ldots,J \text{ and } k=1,\ldots,p, \\
\lambda_{jl} &amp; \sim &amp; \begin{cases}
\mathcal{N}(0,10) &amp; \text{if } l &lt; j \\
\mathcal{U}(0,10) &amp;  \text{if } l=j \\
P \text{ such as } \mathbb{P}(\lambda_{jl} = 0)=1  &amp; \text{if } l&gt;j
\end{cases} \\
\quad &amp;  \quad &amp; \text{ for } j=1,\ldots,J \text{ and } l=1,\ldots,q.
\end{array}\]</span></p>
</div>
<div id="adaptive-metropolis-algorithm-principle" class="section level2">
<h2 class="hasAnchor">
<a href="#adaptive-metropolis-algorithm-principle" class="anchor"></a><span class="header-section-number">2.3</span> Adaptive Metropolis algorithm principle</h2>
<p>This algorithm belongs to the MCMC methods and allows to obtain a realization of the parameter <span class="math inline">\(\theta=(\theta_1,\ldots,,\theta_m)\)</span> according to their conditional <em>a posteriori</em> distributions <span class="math inline">\(\Pi(\theta_i | \theta_1,\dots,\theta_{i-1},\theta_{i+1},\ldots,\theta_m, x)\)</span>, for <span class="math inline">\(i =1,\ldots,m\)</span> known to within a multiplicative constant.<br>
It is called adaptive because the variance of the conditional instrumental density used is adapted according to the number of acceptances in the last iterations.</p>
<ul>
<li><p><strong>Initialization</strong> : <span class="math inline">\(\theta^{(0)}= (\theta_1^{(0)},\ldots,\theta_m^{(0)})\)</span> arbitrarily set, the acceptance numbers <span class="math inline">\((n^A_{i})_{i=1,\ldots,m}\)</span> are initialized at <span class="math inline">\(0\)</span> and the variances <span class="math inline">\((\sigma^2_i)_{i=1,\ldots,m}\)</span> are initialized at <span class="math inline">\(1\)</span>.</p></li>
<li>
<p><strong>Iteration t</strong> : for <span class="math inline">\(i=1,\ldots,m\)</span></p>
<ul>
<li><p>Generate <span class="math inline">\(\theta_i^\star \sim q(\theta_i^{(t-1)},.)\)</span>, with conditional instrumental density
<span class="math inline">\(q(\theta_i^{(t-1)},\theta_i^\star)\)</span> symmetric, we will choose a law <span class="math inline">\(\mathcal{N}(\theta_i^{(t-1)},{\sigma^2_{i}})\)</span> for example.</p></li>
<li><p>Calculate the probability of acceptance :
<span class="math display">\[\gamma=  min\left(1,\dfrac{\Pi\left(\theta_i^\star \ | \ \theta_1^{(t-1)},\dots,\theta_{i-1}^{(t-1)},\theta_{i+1}^{(t-1)},\ldots,\theta_m^{(t-1)}, x \right)}{\Pi\left(\theta_i^{(t-1)} \ | \ \theta_1^{(t-1)},\dots,\theta_{i-1}^{ (t-1)},\theta_{i+1}^{(t-1)},\ldots,\theta_m^{(t-1)},x\right)}\right)\]</span>.</p></li>
<li><p><span class="math display">\[\theta_i^{(t)} =  
\begin{cases} 
\theta_i^\star &amp; \text{ with probability } \gamma \\
&amp;\text{ if we are in this case the acceptance number becomes : } n^A_{i} \leftarrow n^A_{i} +1 \\
\theta_i^{(t-1)} &amp; \text{ with probability } 1-\gamma. \\
\end{cases}\]</span></p></li>
</ul>
</li>
<li>
<p><strong>During the burn-in</strong>, every <span class="math inline">\(\mathrm{DIV}\)</span> iteration, with
<span class="math display">\[\mathrm{DIV} =  \begin{cases} 
100 &amp; \text{ if } N_{Gibbs} \geq 1000 \\
\dfrac{N_{Gibbs}}{10}&amp; \text{ else }  \\
\end{cases}\]</span>
, where <span class="math inline">\(N_{Gibbs}\)</span> is the total number of iterations performed.<br>
The variances are modified as a function of the acceptance numbers as follows for <span class="math inline">\(i=1,\ldots,m\)</span> :</p>
<ul>
<li><p>The acceptance rate is calculated : <span class="math inline">\(r^A_{i} = \dfrac{ n^A_i}{\mathrm{DIV}}\)</span>.</p></li>
<li><p>The variances are adapted according to the acceptance rate and a fixed constant <span class="math inline">\(R_{opt}\)</span> :
<span class="math display">\[\sigma_i \leftarrow \begin{cases}  
\sigma_i\left(2-\dfrac{1-r^A_i}{1-R_{opt}}\right) &amp; \text{ if } r^A_{i} \geq R_{opt} \\ \\
\dfrac{\sigma_i}{2-\dfrac{1-r^A_i}{1-R_{opt}}} &amp; \text{ else }
\end{cases}\]</span></p></li>
<li><p>We reset the acceptance numbers : <span class="math inline">\(n^A_i \leftarrow 0\)</span>.</p></li>
</ul>
</li>
<li><p>Every <span class="math inline">\(\dfrac{N_{Gibbs}}{10}\)</span> iteration, average acceptance rates are calculated and displayed <span class="math inline">\(m^A = \dfrac{1}{m}\sum\limits_{i=1,\ldots,m}r^A_i\)</span>.</p></li>
</ul>
</div>
<div id="gibbs-sampler-using-adaptative-metropolis-algorithm" class="section level2">
<h2 class="hasAnchor">
<a href="#gibbs-sampler-using-adaptative-metropolis-algorithm" class="anchor"></a><span class="header-section-number">2.4</span> Gibbs sampler using adaptative Metropolis algorithm</h2>
<p>An adaptive Metropolis algorithm is used to sample the model parameters according to their conditional <em>a posteriori</em> distributions estimated to within one multiplicative constant.</p>
<p>First we define the <span class="math inline">\(f\)</span> function that calculates the likelihood of the model as a function of the estimated parameters:<br><span class="math display">\[ f : \lambda_j,\beta_{j0},\beta_j,\alpha_i, W_i, X_i, y_{ij},n_i \rightarrow  f(\lambda_j,\beta_{j0},\beta_j,\alpha_i, W_i, X_i, y_{ij},n_i)=\mathrm{L}(\theta_{ij})\]</span>
- Compute <span class="math inline">\(\mathrm{logit}(\theta_{ij})= \alpha_i + \beta_{j0} + X_i\beta_j + W_i\lambda_j\)</span>.</p>
<ul>
<li><p>Compute <span class="math inline">\(\theta_{ij}= \dfrac{1}{1+\exp\left(-\mathrm{logit}(\theta_{ij})\right)}\)</span>.</p></li>
<li><p>Return <span class="math inline">\(\mathrm{L}(\theta_{ij})= p(y_{ij} \ | \ \theta_{ij},n_i)= \dbinom{n_i}{y_{ij}}(\theta_{ij})^{y_{ij}}(1-\theta_{ij})^{n_i-y_{ij}}\)</span>.</p></li>
</ul>
<p>We repeat those steps for <span class="math inline">\(i=1,\ldots,I\)</span> et <span class="math inline">\(j=1,\ldots,J\)</span>, and then we define <span class="math inline">\(\theta = \left(\theta{ij}\right)_{i=1,\ldots I}^{j= 1,\ldots,J}\)</span>.<br>
This allows us to calculate the likelihood of the model: <span class="math inline">\(\mathrm{L}(\theta)= \prod\limits_{\substack{1\leq i\leq I \\ 1 \leq j\leq I}}\mathrm{L}(\theta_{ij})\)</span>.</p>
<p>According to Bayes’ formula we have <span class="math display">\[\mathrm{p}(\theta \ |  \ Y) \propto \Pi(\theta) \mathrm{L}(\theta).\]</span>
We thus use the following relations to approach the conditional <em>a posteriori</em> densities of each of the parameters with <span class="math inline">\(\Pi(.)\)</span> the densities corresponding to their <em>a priori</em> laws.
<span class="math display">\[\begin{aligned}
&amp; p(\beta_{jk} \ |  \ \beta_{j0},\beta_{j1},\ldots,\beta_{jk-1},\beta_{jk+1},\ldots,\beta_{jp}, \lambda_j,\alpha_1,\ldots,\alpha_I, W_1,\ldots,W_I,Y) \propto \Pi(\beta_{jk})\prod\limits_{1\leq i\leq I}  \mathrm{L}(\theta_{ij})\\
&amp;p(\lambda_{jl} \ |  \ \lambda_{j1},\ldots,\lambda_{jl-1},\lambda_{jl+1},\ldots,\lambda_{jq}, \beta_j,\beta_{j0},\alpha_1,\ldots,\alpha_I, W_1,\ldots,W_I,Y) \propto  \Pi(\lambda_{jl}) \prod\limits_{1\leq i \leq I}\mathrm{L}(\theta_{ij})\\
&amp;p(W_{il} \ |  \ W_{i1},\ldots,W_{il-1},W_{il+1},\ldots,W_{iq},\alpha_i,\beta_{10},\ldots,\beta_{J0},\beta_1,\ldots,\beta_J,\lambda_1,\ldots, \lambda_J,Y) \propto \Pi(W_{il}) \prod\limits_{1\leq j\leq J}\mathrm{L}(\theta_{ij})\\
&amp;p(\alpha_i \ |  \ W_i,\beta_{10},\ldots,\beta_{J0},\beta_1,\ldots,\beta_J,\lambda_1,\ldots, \lambda_j,V_{\alpha},Y) \propto \Pi(\alpha_i \ | \ V_{\alpha}) \prod\limits_{1\leq j\leq J}\mathrm{L}(\theta_{ij})\\
&amp; \text{, for $i=1,\ldots,I$, $j=1,\ldots,J$, $k=1,\ldots,p$ and $l=1,\ldots,q$. 
}
\end{aligned}\]</span></p>
<p>The algorithm implemented in  on the basis of <span class="citation">Rosenthal (<a href="#ref-Rosenthal2009" role="doc-biblioref">2009</a>)</span> and <span class="citation">Roberts &amp; Rosenthal (<a href="#ref-Roberts2001" role="doc-biblioref">2001</a>)</span> articles, to estimate the parameters of the logit model is the following :</p>
<ul>
<li><p>Definition of constants <span class="math inline">\(N_{Gibbs}\)</span>, <span class="math inline">\(N_{burn}\)</span>, <span class="math inline">\(N_{thin}\)</span> and <span class="math inline">\(R_{opt}\)</span> such that <span class="math inline">\(N_{Gibbs}\)</span> corresponds to the number of iterations performed by the algorithm, <span class="math inline">\(N_{burn}\)</span> to the number of iterations required for the burn-in or warm-up time,<br><span class="math inline">\(N_{samp}= \dfrac{N_{Gibbs}-N_{burn}}{N_{thin}}\)</span> corresponding to the number of estimated values retained for each parameter. Indeed we record the estimated parameters at certain iterations in order to obtain <span class="math inline">\(N_{samp}\)</span> values, allowing us to represent a <span class="math inline">\(a \ posteriori\)</span> distribution for each parameter.<br>
We set <span class="math inline">\(R_{opt}\)</span> the optimal acceptance ratio used in the adaptive Metropolis algorithms implemented for each parameter of the model.</p></li>
<li><p>Initialize all parameters to <span class="math inline">\(0\)</span> for example, except the diagonal values of <span class="math inline">\(\Lambda\)</span> initialized at <span class="math inline">\(1\)</span> and <span class="math inline">\(V_{\alpha}^{(0)}=1\)</span>. The acceptance number of each parameter is initialized to <span class="math inline">\(0\)</span> and the variances of their conditional instrumental densities take the value <span class="math inline">\(1\)</span>.</p></li>
<li>
<p><strong>Gibbs sampler</strong> at each iteration <span class="math inline">\(t\)</span> for <span class="math inline">\(t=1,\ldots,N_{Gibbs}\)</span> we repeat each of these steps:</p>
<ul>
<li><p>Generate the <strong>random site effects</strong> <span class="math inline">\(\alpha_i^{(t)}\)</span> for <span class="math inline">\(i=1,\ldots,I\)</span> according to an adaptive Metropolis algorithm that simulates <span class="math inline">\(\alpha_i^\star \sim \mathcal{N}(\alpha_i^{(t-1)},\sigma_{\alpha_i}^2)\)</span> and then calculates the acceptance rate as follows:<br><span class="math display">\[\gamma =min\left(1, \ \dfrac{\Pi\left(\alpha_i^\star \ | \ V_{\alpha}^{(t-1)}\right)\prod\limits_{1\leq j\leq J}f\left(\alpha_i^\star, W_i^{(t-1)},\beta_{j0}^{(t-1)},\beta_j^{(t-1)}, \lambda_j^{(t-1)}, X_i,y_{ij},n_i\right)}{\Pi\left(\alpha_i^{(t-1)} \ | \ V_{\alpha}^{(t-1)}\right)\prod\limits_{1\leq j\leq J}f\left(\alpha_i^{(t-1)}, W_i^{(t-1)},\beta_{j0}^{(t-1)},\beta_j^{(t-1)}, \lambda_j^{(t-1)}, X_i,y_{ij},n_i\right)}\right).\]</span></p></li>
<li><p>Generate the <strong>variance of random site effects</strong> <span class="math inline">\(V_\alpha^{(t)}\)</span> according to: <span class="math display">\[V_\alpha^{(t)} \ | \ \alpha_1^{(t)},\ldots,\alpha_I^{(t)} \sim \mathcal{IG}\left( \text{shape}=0.5 + \frac{I}{2}, \text{rate}=0.005 + \frac{1}{2}\sum\limits_{i=1}^I \left(\alpha_i^{(t)}\right)^2\right)\]</span></p></li>
<li><p>Generate the <strong>latent variables</strong> (or unmeasured predictors) <span class="math inline">\(W_{il}^{(t)}\)</span> for <span class="math inline">\(i=1,\ldots,I\)</span> and <span class="math inline">\(l=1,\ldots,q\)</span> according to an adaptive Metropolis algorithm that simulates <span class="math inline">\(W_{il}^\star \sim \mathcal{N}(W_{il}^{(t-1)}, \sigma_{W_{il}}^2)\)</span>and then calculates the acceptance rate as follows:</p></li>
</ul>
</li>
</ul>
<p><span class="math display">\[\gamma = min\left(1,\ \dfrac{\Pi\left(W_{il}^\star\right)\prod\limits_{1\leq j\leq J}f\left(W_{il}^\star, \alpha_i^{(t)},\beta_{j0}^{(t-1)},\beta_j^{(t-1)}, \lambda_j^{(t-1)},X_i,y_{ij},n_i\right)} {\Pi\left(W_{il}^{(t-1)}\right)\prod\limits_{1\leq j\leq J}f\left(W_{il}^{(t-1)}, \alpha_i^{(t)},\beta_{j0}^{(t-1)},\beta_j^{(t-1)}, \lambda_j^{(t-1)}, X_i,y_{ij},n_i\right)}\right).\]</span></p>
<ul>
<li>Generate the <strong>fixed species effects</strong> <span class="math inline">\(\beta_{jk}^{(t)}\)</span> for <span class="math inline">\(j=1,\ldots,J\)</span> and <span class="math inline">\(k=0,\ldots,p\)</span> using an adaptive Metropolis algorithm that simulates <span class="math inline">\(\beta_{jk}^\star \sim \mathcal{N}(\beta_{jk}^{(t-1)}, \sigma_{\beta_{jk}}^2)\)</span> and then calculates the acceptance rate as follows:</li>
</ul>
<p><span class="math display">\[\gamma = min\left(1,\dfrac{\Pi\left(\beta_{jk}^\star\right)\prod\limits_{1\leq i\leq I}f\left(\beta_{j0}^{(t)},\small{\ldots},\beta_{jk-1}^{(t)},\beta_{jk}^\star,\beta_{jk+1}^{(t-1)},\small{\ldots}, \beta_{jp}^{(t-1)},\lambda_j^{(t-1)}, \alpha_1^{(t)},W_1^{(t)},\small{\ldots},\alpha_I^{(t)},  W_I^{(t)},X_i,y_{ij},n_i\right)} {\Pi\left(\beta_{jk}^{(t-1)}\right)\prod\limits_{1\leq i\leq I}f\left(\beta_{j0}^{(t)},\small{\ldots},\beta_{jk-1}^{(t)},\beta_{jk}^{(t-1)},\beta_{jk+1}^{(t-1)},\small{\ldots}, \beta_{jp}^{(t-1)},\lambda_j^{(t-1)}, \alpha_1^{(t)},W_1^{(t)}, \small{\ldots},\alpha_I^{(t)},  W_I^{(t)},X_i,y_{ij},n_i\right)}\right).\]</span></p>
<ul>
<li>Generate the <strong>loading factors related to latent variables</strong> <span class="math inline">\(\lambda_{jl}^{(t)}\)</span> for <span class="math inline">\(j=1,\ldots,J\)</span> and <span class="math inline">\(l=1,\ldots,q\)</span> according to an adaptive Metropolis algorithm for <span class="math inline">\(l \leq j\)</span>, simulating <span class="math inline">\(\lambda_{jl}^\star \sim \mathcal{N}(\lambda_{jl}^{(t-1)},\sigma_{\lambda_{jl}}^2)\)</span> and then calculating the acceptance rate as follows: :
<span class="math display">\[\gamma = min\left(1,\dfrac{\Pi\left(\lambda_{jl}^\star\right)\prod\limits_{1\leq i\leq I}f\left(\lambda_{j1}^{(t)},\small{\ldots},\lambda_{jl-1}^{(t)},\lambda_{jl}^\star,\lambda_{jl+1}^{(t-1)},\small{\ldots}, \lambda_{jq}^{(t-1)},\beta_{j0}^{(t)},\beta_j^{(t)}, \alpha_1^{(t)},W_1^{(t)},\small{\ldots},\alpha_I^{(t)},  W_I^{(t)},X_i,y_{ij},n_i\right)} {\Pi\left(\lambda_{jl}^{(t-1)}\right)\prod\limits_{1\leq i\leq I}f\left(\lambda_{j1}^{(t)},\small{\ldots},\lambda_{jl-1}^{(t)},\lambda_{jl}^{(t-1)},\lambda_{jl+1}^{(t-1)},\small{\ldots}, \lambda_{jq}^{(t-1)},\beta_{j0}^{(t)},\beta_j^{(t)}, \alpha_1^{(t)},W_1^{(t)},\small{\ldots},\alpha_I^{(t)},  W_I^{(t)},X_i,y_{ij},n_i\right)}\right).\]</span>
In the case of <span class="math inline">\(l&gt;j\)</span>, we put <span class="math inline">\(\lambda_{jl}^{(t)} = 0\)</span>.</li>
</ul>
</div>
</div>
<div id="poisson-distribution-with-log-link-function" class="section level1">
<h1 class="hasAnchor">
<a href="#poisson-distribution-with-log-link-function" class="anchor"></a><span class="header-section-number">3</span> Poisson distribution with log link function</h1>
<div id="model-definition-2" class="section level2">
<h2 class="hasAnchor">
<a href="#model-definition-2" class="anchor"></a><span class="header-section-number">3.1</span> Model definition</h2>
<p>According to the article <span class="citation">Hui (<a href="#ref-Hui2016" role="doc-biblioref">2016</a>)</span>, we can use the Poisson distribution for the analysis of multivariate abundance data, with estimation performed using Bayesian Markov chain Monte Carlo methods.</p>
<p>In this case, it is assumed that <span class="math display">\[y_{ij} \sim \mathcal{P}oisson(\theta_{ij})\]</span>, with
<span class="math inline">\(\mathrm{log}(\theta_{ij}) = \alpha_i + \beta_{j0}+X_i\beta_j+ W_i\lambda_j\)</span>.</p>
<p>We therefore consider abundance data with a response variable noted : <span class="math inline">\(Y=(y_{ij})^{i=1,\ldots,nsite}_{j=1,\ldots,nsp}\)</span> such as :</p>
<p><span class="math display">\[y_{ij}=\begin{cases}
    0 &amp; \text{if species $j$ has been observed as absent at site $i$}\\
    n &amp;  \text{if $n$ individuals of the species $j$ have been observed at the site $i$}.
    \end{cases}\]</span></p>
</div>
<div id="gibbs-sampler-using-adaptative-metropolis-algorithm-1" class="section level2">
<h2 class="hasAnchor">
<a href="#gibbs-sampler-using-adaptative-metropolis-algorithm-1" class="anchor"></a><span class="header-section-number">3.2</span> Gibbs sampler using adaptative Metropolis algorithm</h2>
<p>In this case, we cannot use the properties of the conjugate priors, therefore, the parameters of this model will be sampled by estimating their conditional <em>a posteriori</em> distributions using an adaptive Metropolis algorithm in the Gibbs sampler, in the same way as for the logit model.</p>
<p>We use the same algorithm as before by replacing the logit link function by a log link function and the binomial distribution by a poisson’s law to calculate the likelihood of the model in the function .</p>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h1>
<div id="refs" class="references">
<div id="ref-Albert1993">
<p>Albert, J.H. &amp; Siddhartha, C. (1993) Bayesian analysis of binary and polychotomous response data. <em>Journal of the American Statistical Association</em>, <strong>88</strong>, 669–679.</p>
</div>
<div id="ref-Hui2016">
<p>Hui, F.K.C. (2016) Boral – Bayesian Ordination and Regression Analysis of Multivariate Abundance Data in r. <em>Methods in Ecology and Evolution</em>, <strong>7</strong>, 744–750.</p>
</div>
<div id="ref-Roberts2001">
<p>Roberts, G.O. &amp; Rosenthal, J.S. (2001) Optimal scaling for various Metropolis-Hastings algorithms. <em>Statistical Science</em>, <strong>16</strong>, 351–367.</p>
</div>
<div id="ref-Rosenthal2009">
<p>Rosenthal, S. (2009) Optimal Proposal Distributions and Adaptive MCMC. <em>Handbook of Markov Chain Monte Carlo</em>.</p>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by Jeanne Clément, <a href="https://ecology.ghislainv.fr">Ghislain Vieilledent</a>, <a href="https://www.cirad.fr"><img src="https://ecology.ghislainv.fr/images/logos/logo-cirad.svg" height="24"></a>.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
